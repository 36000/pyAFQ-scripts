{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudknot as ck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ck.set_region('us-east-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def afq_prek(subject):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import s3fs\n",
    "    import json\n",
    "    import logging\n",
    "    import os.path as op\n",
    "    import nibabel as nib\n",
    "    import dipy.data as dpd\n",
    "    import dipy.tracking.utils as dtu\n",
    "    import dipy.tracking.streamline as dts\n",
    "    from dipy.io.streamline import save_tractogram, load_tractogram\n",
    "    from dipy.stats.analysis import afq_profile, gaussian_weights\n",
    "    from dipy.io.stateful_tractogram import StatefulTractogram\n",
    "    from dipy.io.stateful_tractogram import Space\n",
    "    import dipy.core.gradients as dpg\n",
    "    from dipy.segment.mask import median_otsu\n",
    "\n",
    "    import AFQ.data as afd\n",
    "    import AFQ.tractography as aft\n",
    "    import AFQ.registration as reg\n",
    "    import AFQ.dti as dti\n",
    "    import AFQ.segmentation as seg\n",
    "    from AFQ import api\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    log = logging.getLogger(__name__)    \n",
    "        \n",
    "    dwi_dir = f'prek-diffusion/derivatives/sub-{subject}/dtiInit_ses-pre'\n",
    "\n",
    "    anat_dir = f'prek-diffusion/derivatives/sub-{subject}/freesurfer/mri'\n",
    "\n",
    "    prek_fdata = op.join(dwi_dir, f\"sub-{subject}_ses-pre_acq-b1500_dwi_aligned_trilin.nii.gz\")\n",
    "    prek_fbval = op.join(dwi_dir, f\"sub-{subject}_ses-pre_acq-b1500_dwi_aligned_trilin.bvals\")\n",
    "    prek_fbvec = op.join(dwi_dir, f\"sub-{subject}_ses-pre_acq-b1500_dwi_aligned_trilin.bvecs\")\n",
    "\n",
    "    log.info(f\"Reading data from file {prek_fdata}\")\n",
    "    img = nib.load(prek_fdata)\n",
    "    log.info(f\"Creating gradient table from {prek_fbval} and {prek_fbvec}\")\n",
    "    gtab = dpg.gradient_table(prek_fbval, prek_fbvec)\n",
    "    \n",
    "    bucket_name = f'prek-diffusion/derivatives/sub-{subject}/pyafq'\n",
    "    fs = s3fs.S3FileSystem()\n",
    "    \n",
    "    wm_mask_fname = f'{bucket_name}/sub-{subject}_wm_mask.nii.gz'\n",
    "    if fs.exists(wm_mask_fname):\n",
    "        log.info(f\"WM mask exists. Reading from {wm_mask_fname}\")\n",
    "        wm_img = afd.s3fs_nifti_read(wm_mask_fname)\n",
    "        wm_mask = wm_img.get_data()\n",
    "    else:\n",
    "        log.info(f\"Calculating WM segmentation\")\n",
    "        wm_labels=[250, 251, 252, 253, 254, 255, 41, 2, 16, 77]\n",
    "        seg_img = nib.load(op.join(anat_dir, f\"aparc+aseg.nii.gz\"))\n",
    "        seg_data_orig = seg_img.get_fdata()\n",
    "        # For different sets of labels, extract all the voxels that\n",
    "        # have any of these values:\n",
    "        wm_mask = np.sum(np.concatenate([(seg_data_orig == l)[..., None]\n",
    "                                        for l in wm_labels], -1), -1)\n",
    "\n",
    "        # Resample to DWI data:\n",
    "        dwi_data = img.get_fdata()\n",
    "        wm_mask = np.round(reg.resample(wm_mask, \n",
    "                                        dwi_data[..., 0],\n",
    "                                        seg_img.affine,\n",
    "                                        img.affine)).astype(int)\n",
    "\n",
    "        wm_img = nib.Nifti1Image(wm_mask.astype(int),\n",
    "                                 img.affine)\n",
    "        log.info(f\"Saving to {wm_mask_fname}\")\n",
    "        afd.s3fs_nifti_write(wm_img, wm_mask_fname)\n",
    "    \n",
    "    fa_fname = f'{bucket_name}/sub-{subject}_dti_FA.nii.gz'\n",
    "    dti_params_fname = f'{bucket_name}/sub-{subject}_dti.nii.gz'\n",
    "    dti_meta_fname = f'{bucket_name}/sub-{subject}_dti.json'\n",
    "    if fs.exists(fa_fname):\n",
    "        log.info(f\"DTI already exists. Reading FA from {fa_fname}\")\n",
    "        log.info(f\"DTI already exists. Reading params from {dti_params_fname}\")\n",
    "        FA_img = afd.s3fs_nifti_read(fa_fname)\n",
    "        dti_params = afd.s3fs_nifti_read(dti_params_fname)\n",
    "    else:\n",
    "        log.info(\"Calculating DTI\")\n",
    "        dti_params = dti.fit_dti(prek_fdata, prek_fbval, prek_fbvec,\n",
    "                                out_dir='.', b0_threshold=50,\n",
    "                                mask=wm_mask)\n",
    "        FA_img = nib.load('./dti_FA.nii.gz')\n",
    "        log.info(f\"Writing FA to {fa_fname}\")\n",
    "        afd.s3fs_nifti_write(FA_img, fa_fname)\n",
    "        dti_params_img = nib.load('./dti_params.nii.gz')\n",
    "        log.info(f\"Writing DTI params to {dti_params_fname}\")\n",
    "        afd.s3fs_nifti_write(dti_params_img, dti_params_fname)\n",
    "        dti_params_json = {\"Model\": \"Diffusion Tensor\",\n",
    "                           \"OrientationRepresentation\": \"param\",\n",
    "                            \"ReferenceAxes\": \"xyz\",\n",
    "                            \"Parameters\": {\n",
    "                                \"FitMethod\": \"ols\",\n",
    "                                \"OutlierRejection\": False\n",
    "                                }\n",
    "                          }\n",
    "        log.info(f\"Writing DTI metadata to {dti_meta_fname}\")\n",
    "        afd.s3fs_json_write(dti_params_json, dti_meta_fname)\n",
    "\n",
    "    log.info(f\"Reading FA data from img\")\n",
    "    FA_data = FA_img.get_fdata()\n",
    "\n",
    "    dti_streamlines_fname = f'{bucket_name}/sub-{subject}_model-dti_track-det.trk'\n",
    "    dti_streamlines_meta_fname = f'{bucket_name}/sub-{subject}_model-dti_track-det.json'\n",
    "    if fs.exists(dti_streamlines_fname):\n",
    "        log.info(f\"Streamlines already exist. Loading from {dti_streamlines_fname}\")        \n",
    "        fs.download(dti_streamlines_fname, './dti_streamlines.trk')\n",
    "        tg = load_tractogram('./dti_streamlines.trk', img)\n",
    "        streamlines = tg.streamlines\n",
    "    else:\n",
    "        log.info(f\"Generating streamlines\")      \n",
    "        seed_roi = np.zeros(img.shape[:-1])\n",
    "        seed_roi[FA_data > 0.4] = 1\n",
    "        seed_roi[wm_mask < 1] = 0\n",
    "        streamlines = aft.track(dti_params_img, seed_mask=seed_roi,\n",
    "                                directions='det', stop_mask=FA_data,\n",
    "                                stop_threshold=0.1)\n",
    "        log.info(f\"After tracking, there are {len(streamlines)} streamlines\")\n",
    "        sft = StatefulTractogram(streamlines, img, Space.RASMM)\n",
    "        save_tractogram(sft, './dti_streamlines.trk',\n",
    "                        bbox_valid_check=False)\n",
    "        log.info(f\"Uploading streamlines to {dti_streamlines_fname}\")\n",
    "        fs.upload('./dti_streamlines.trk', dti_streamlines_fname)\n",
    "        dti_streamlines_json = {\n",
    "            \"Algorithm\" : \"LocalTracking\",\n",
    "            \"AlgorithmURL\":\"https://github.com/yeatmanlab/pyAFQ/\",\n",
    "            \"Parameters\":{\n",
    "            \"SeedRoi\": \"dti_FA>0.4\",\n",
    "            \"Directions\": \"det\",\n",
    "            \"StopMask\" : \"dti_FA<0.1\"}\n",
    "            }\n",
    "        log.info(f\"Writing streamlines metadata to {dti_streamlines_meta_fname}\")\n",
    "        afd.s3fs_json_write(dti_streamlines_json, dti_streamlines_meta_fname)\n",
    "    \n",
    "    streamlines = dts.Streamlines(\n",
    "            dtu.transform_tracking_output(streamlines,\n",
    "                                  np.linalg.inv(img.affine)))\n",
    " \n",
    "    log.info(\"Segmenting\")\n",
    "        \n",
    "    # Use the default for waypoint ROI\n",
    "    bundles = api.make_bundle_dict()\n",
    "\n",
    "    segmentation = seg.Segmentation(b0_threshold=50,\n",
    "                                    prob_threshold=10,\n",
    "                                    return_idx=True)\n",
    "    segmentation.segment(bundles, \n",
    "                         streamlines, \n",
    "                         fdata=prek_fdata,\n",
    "                         fbval=prek_fbval,\n",
    "                         fbvec=prek_fbvec)\n",
    "\n",
    "    fiber_groups = segmentation.fiber_groups\n",
    "\n",
    "    sl_count = []\n",
    "    for kk in fiber_groups:\n",
    "        log.info(f\"Cleaning {kk}\")\n",
    "        len_before = len(fiber_groups[kk]['sl'])\n",
    "        log.info(f\"Before cleaning there are {len_before} streamlines\")\n",
    "        new_fibers, idx_in_bundle = seg.clean_fiber_group(\n",
    "                            fiber_groups[kk]['sl'],\n",
    "                            return_idx=True, \n",
    "                            clean_threshold=3)\n",
    "\n",
    "        log.info(f\"After cleaning there are {len(new_fibers)} streamlines\")\n",
    "        idx_in_global = fiber_groups[kk]['idx'][idx_in_bundle]\n",
    "        \n",
    "        sl_count.append(len(new_fibers))\n",
    "        log.info(f\"There are {sl_count[-1]} streamlines in {kk}\")\n",
    "        sft = StatefulTractogram(\n",
    "            dtu.transform_tracking_output(new_fibers, img.affine),\n",
    "            img, Space.RASMM)\n",
    "\n",
    "        local_tg_fname = './%s_afq.trk'%kk\n",
    "        save_tractogram(sft, local_tg_fname,\n",
    "                        bbox_valid_check=False)\n",
    "        tg_fname = f'{bucket_name}/sub-{subject}_model-dti_track-det_segment-afq_bundle-{kk}.trk'\n",
    "        log.info(f\"Uploading {local_tg_fname} to {tg_fname}\")\n",
    "        fs.upload('./%s_afq.trk'%kk, tg_fname)\n",
    "        tg_meta_fname = f'{bucket_name}_model-dti_track-det_segment-afq_bundle-{kk}.json'\n",
    "        tg_meta_json = {\n",
    "            \"Algorithm\" : \"AFQ\",\n",
    "            \"AlgorithmURL\" : \"https://github.com/yeatmanlab/pyAFQ/\",\n",
    "            \"Parameters\":\n",
    "            {\"clean_threshold\":3,\n",
    "             \"prob_threshold\": 10}\n",
    "        }\n",
    "        \n",
    "        log.info(f\"Uploading segmentation metadata to {tg_meta_fname}\")\n",
    "        afd.s3fs_json_write(tg_meta_json, tg_meta_fname)\n",
    "\n",
    "        np.save('bundle_idx.npy', idx_in_global)\n",
    "        idx_fname = f'{bucket_name}_model-dti_track-det_segment-afq_bundle-{kk}_idx.npy'\n",
    "        log.info(f\"Uploading bundle indices to {idx_fname}\")\n",
    "        fs.upload('bundle_idx.npy', idx_fname)\n",
    "\n",
    "    log.info(\"Saving streamline counts\")\n",
    "    sl_count = pd.DataFrame(data=sl_count, index=fiber_groups.keys(), columns=[\"streamlines\"])\n",
    "    sl_count.to_csv(\"./sl_count.csv\")\n",
    "    sl_count_fname = f'{bucket_name}_model-dti_track-det_segment-afq_counts.csv'\n",
    "    log.info(f\"Uploading streamline counts to {sl_count_fname}\")\n",
    "    fs.upload(\"./sl_count.csv\", sl_count_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "BuildError",
     "evalue": "The command '/bin/sh -c pip install --no-cache-dir -r /tmp/requirements.txt     && pip install --no-cache-dir git+https://github.com/yeatmanlab/pyAFQ.git' returned a non-zero code: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBuildError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f515997505bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                   \u001b[0mresource_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"SPOT\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                   \u001b[0mbid_percentage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                   memory=64000)\n\u001b[0m",
      "\u001b[0;32m~/cloudknot/cloudknot/cloudknot.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, pars, pars_policies, docker_image, base_image, func, image_script_path, image_work_dir, image_github_installs, username, repo_name, image_tags, job_definition_name, job_def_vcpus, memory, retries, compute_environment_name, instance_types, resource_type, min_vcpus, max_vcpus, desired_vcpus, image_id, ec2_key_pair, bid_percentage, job_queue_name, priority)\u001b[0m\n\u001b[1;32m   1483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m             self._docker_image, self._docker_repo, repo_cleanup = futures[\n\u001b[0;32m-> 1485\u001b[0;31m                 \u001b[0;34m\"docker-image\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1486\u001b[0m             ].result()\n\u001b[1;32m   1487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/afq/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    426\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/afq/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/afq/lib/python3.7/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cloudknot/cloudknot/cloudknot.py\u001b[0m in \u001b[0;36mset_dockerimage\u001b[0;34m(knot_name, input_docker_image, func_, script_path, work_dir, base_image_, github_installs, username_, tags, repo_name_)\u001b[0m\n\u001b[1;32m   1410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1412\u001b[0;31m                     \u001b[0mdi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1413\u001b[0m                     mod_logger.info(\n\u001b[1;32m   1414\u001b[0m                         \u001b[0;34m\"knot {name:s} built docker image {i!s}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cloudknot/cloudknot/dockerimage.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, tags, image_name)\u001b[0m\n\u001b[1;32m    507\u001b[0m                 \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m                 \u001b[0mdockerfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocker_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m                 \u001b[0mtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\":\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tag\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m             )\n\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/afq/lib/python3.7/site-packages/docker/models/images.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minternal_stream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mBuildError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'stream'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m                 match = re.search(\n",
      "\u001b[0;31mBuildError\u001b[0m: The command '/bin/sh -c pip install --no-cache-dir -r /tmp/requirements.txt     && pip install --no-cache-dir git+https://github.com/yeatmanlab/pyAFQ.git' returned a non-zero code: 1"
     ]
    }
   ],
   "source": [
    "afq_knot = ck.Knot(name='afq_prek-64gb-191202-02',\n",
    "                  func=afq_prek,\n",
    "                  image_github_installs=\"https://github.com/yeatmanlab/pyAFQ.git\",\n",
    "                  pars_policies=('AmazonS3FullAccess',),\n",
    "                  resource_type=\"SPOT\",\n",
    "                  bid_percentage=100,\n",
    "                  memory=64000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [1110,\n",
    "          1112,\n",
    "          1113]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = afq_knot.map(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID              Name                        Status   \n",
      "---------------------------------------------------------\n",
      "a3f9cf77-7504-4c22-a3ba-946a10400867        afq-hcp-64gb-191101-27-0        SUBMITTED\n"
     ]
    }
   ],
   "source": [
    "afq_knot.view_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "j0 = afq_knot.jobs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'PENDING',\n",
       " 'statusReason': None,\n",
       " 'attempts': [],\n",
       " 'arrayProperties': {'statusSummary': {'STARTING': 1,\n",
       "   'FAILED': 0,\n",
       "   'RUNNING': 18,\n",
       "   'SUCCEEDED': 0,\n",
       "   'RUNNABLE': 2,\n",
       "   'SUBMITTED': 0,\n",
       "   'PENDING': 0},\n",
       "  'size': 21}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j0.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# afq_knot.clobber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
