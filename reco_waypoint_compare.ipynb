{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hcp.recobundles/sub-2/sub-3_csd.nii.gz'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'hcp.recobundles' + f'/sub-{2}/sub-{3}_csd.nii.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is already in place. If you want to fetch it again please first remove the folder /Users/john/AFQ_data/templates \n",
      "Dataset is already in place. If you want to fetch it again please first remove the folder /Users/john/AFQ_data/callosum_templates \n",
      "{'IFO_R', 'CGC_L', 'FP', 'ATR_L', 'UNC_L', 'FA', 'CST_R', 'HCC_R', 'SLF_L', 'CST_L', 'ILF_R', 'ILF_L', 'IFO_L', 'ATR_R', 'SLF_R', 'HCC_L', 'ARC_R', 'UNC_R', 'CGC_R', 'ARC_L'}\n"
     ]
    }
   ],
   "source": [
    "from AFQ.api import make_bundle_dict\n",
    "bundles_waypoint_roi = set(make_bundle_dict().keys())\n",
    "print(bundles_waypoint_roi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is already in place. If you want to fetch it again please first remove the folder /Users/john/AFQ_data/hcp_atlas_16_bundles \n",
      "{'AF_L', 'F_R', 'CST_L', 'IFOF_R', 'F_L', 'UF_R', 'C_L', 'CST_R', 'UF_L', 'CCMid', 'MCP', 'whole_brain', 'CC_ForcepsMajor', 'CC_ForcepsMinor', 'AF_R', 'C_R', 'IFOF_L'}\n"
     ]
    }
   ],
   "source": [
    "from AFQ.data import read_hcp_atlas_16_bundles\n",
    "bundles_reco = set(read_hcp_atlas_16_bundles().keys())\n",
    "print(bundles_reco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CST_L', 'CST_R'}\n"
     ]
    }
   ],
   "source": [
    "print(bundles_waypoint_roi & bundles_reco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IFOF is IFO\n",
    "# UNC is UF\n",
    "# AF is ARC\n",
    "# CST is CST\n",
    "# FA and FP maybe related to F_L and F_R\n",
    "\n",
    "# F-1 score comparing set membership\n",
    "# dice coefficient between covered voxels (1 or 0 map)\n",
    "# compare bundle profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os.path as op\n",
    "\n",
    "CP = configparser.ConfigParser()\n",
    "CP.read_file(open(op.join(op.expanduser('~'), '.aws', 'credentials')))\n",
    "CP.sections()\n",
    "ak = CP.get('hcp', 'AWS_ACCESS_KEY_ID')\n",
    "sk = CP.get('hcp', 'AWS_SECRET_ACCESS_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def clean_fiber_group(streamlines, n_points=100, clean_rounds=5,\n",
    "                      clean_threshold=3, min_sl=20, stat=np.mean):\n",
    "    \"\"\"\n",
    "    Clean a segmented fiber group based on the Mahalnobis distance of\n",
    "    each streamline\n",
    "    Parameters\n",
    "    ----------\n",
    "    streamlines : nibabel.Streamlines class instance.\n",
    "        The streamlines constituting a fiber group.\n",
    "        If streamlines is None, will use previously given streamlines.\n",
    "        Default: None.\n",
    "    clean_rounds : int, optional.\n",
    "        Number of rounds of cleaning based on the Mahalanobis distance from\n",
    "        the mean of extracted bundles. Default: 5\n",
    "    clean_threshold : float, optional.\n",
    "        Threshold of cleaning based on the Mahalanobis distance (the units are\n",
    "        standard deviations). Default: 3.\n",
    "    min_sl : int, optional.\n",
    "        Number of streamlines in a bundle under which we will\n",
    "        not bother with cleaning outliers. Default: 20.\n",
    "    stat : callable, optional.\n",
    "        The statistic of each node relative to which the Mahalanobis is\n",
    "        calculated. Default: `np.mean` (but can also use median, etc.)\n",
    "    Returns\n",
    "    -------\n",
    "    A nibabel.Streamlines class instance containing only the streamlines\n",
    "    that have a Mahalanobis distance smaller than `clean_threshold` from\n",
    "    the mean of each one of the nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    # We don't even bother if there aren't enough streamlines:\n",
    "    if len(streamlines) < min_sl:\n",
    "        return streamlines\n",
    "\n",
    "    # Resample once up-front:\n",
    "    fgarray = _resample_bundle(streamlines[:]['sl'], n_points)\n",
    "    # Keep this around, so you can use it for indexing at the very end:\n",
    "    idx = np.arange(len(fgarray))\n",
    "    # This calculates the Mahalanobis for each streamline/node:\n",
    "    w = gaussian_weights(fgarray, return_mahalnobis=True, stat=stat)\n",
    "    # We'll only do this for clean_rounds\n",
    "    rounds_elapsed = 0\n",
    "    while (np.any(w > clean_threshold)\n",
    "           and rounds_elapsed < clean_rounds\n",
    "           and len(streamlines) > min_sl):\n",
    "        # Select the fibers that have Mahalanobis smaller than the\n",
    "        # threshold for all their nodes:\n",
    "        idx_belong = np.where(\n",
    "            np.all(w < clean_threshold, axis=-1))[0]\n",
    "        idx = idx[idx_belong.astype(int)]\n",
    "        # Update by selection:\n",
    "        fgarray = fgarray[idx_belong.astype(int)]\n",
    "        # Repeat:\n",
    "        w = gaussian_weights(fgarray, return_mahalnobis=True)\n",
    "        rounds_elapsed += 1\n",
    "    # Select based on the variable that was keeping track of things for us:\n",
    "    return streamlines[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyAFQ examples. Performs tractography on HARDI dataset.\n",
    "\"\"\"\n",
    "==========================\n",
    "Plotting tract profiles\n",
    "==========================\n",
    "\n",
    "An example of tracking and segmenting two tracts, and plotting their tract\n",
    "profiles for FA (calculated with DTI).\n",
    "\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import os.path as op\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import dipy.data as dpd\n",
    "from dipy.data import fetcher\n",
    "import dipy.tracking.utils as dtu\n",
    "import dipy.tracking.streamline as dts\n",
    "from dipy.io.streamline import save_tractogram, load_tractogram\n",
    "from dipy.stats.analysis import afq_profile, gaussian_weights\n",
    "from dipy.io.stateful_tractogram import StatefulTractogram\n",
    "from dipy.io.stateful_tractogram import Space\n",
    "\n",
    "import AFQ.utils.streamlines as aus\n",
    "import AFQ.data as afd\n",
    "import AFQ.tractography as aft\n",
    "import AFQ.registration as reg\n",
    "import AFQ.dti as dti\n",
    "import AFQ.segmentation as seg\n",
    "from AFQ.utils.volume import patch_up_roi\n",
    "from AFQ.api import make_bundle_dict\n",
    "\n",
    "def profile(subject):\n",
    "    bundle_names_afq = ['CST',\n",
    "                        'UNC',\n",
    "                        'ARC',\n",
    "                        'IFO']\n",
    "    bundles_afq = make_bundle_dict(bundle_names=bundle_names_afq, seg_algo='afq')\n",
    "    print(f\"Fetching HCP subject {subject}\")\n",
    "    afd.fetch_hcp([subject], \n",
    "                  profile_name=False,\n",
    "                  aws_access_key_id=ak,\n",
    "                  aws_secret_access_key=sk)  \n",
    "    dwi_dir = op.join(afd.afq_home, 'HCP', 'derivatives',\n",
    "                      'dmriprep', f'sub-{subject}', 'sess-01/dwi')\n",
    "\n",
    "    hardi_fdata = op.join(dwi_dir, f\"sub-{subject}_dwi.nii.gz\")\n",
    "    hardi_fbval = op.join(dwi_dir, f\"sub-{subject}_dwi.bval\")\n",
    "    hardi_fbvec = op.join(dwi_dir, f\"sub-{subject}_dwi.bvec\")\n",
    "\n",
    "    img = nib.load(hardi_fdata)\n",
    "\n",
    "    print(\"Calculating DTI...\")\n",
    "    if not op.exists(f'./{subject}/dti_FA.nii.gz'):\n",
    "        dti_params = dti.fit_dti(hardi_fdata, hardi_fbval, hardi_fbvec,\n",
    "                                 out_dir=f'./{subject}')\n",
    "    else:\n",
    "        dti_params = {'FA': f'./{subject}/dti_FA.nii.gz',\n",
    "                      'params': f'./{subject}/dti_params.nii.gz'}\n",
    "\n",
    "    FA_img = nib.load(dti_params['FA'])\n",
    "    FA_data = FA_img.get_fdata()\n",
    "\n",
    "    templates = afd.read_templates()\n",
    "    bundles = make_bundle_dict()\n",
    "\n",
    "    print(\"Registering to template...\")\n",
    "    MNI_T2_img = dpd.read_mni_template()\n",
    "    if not op.exists(f'./{subject}/mapping.nii.gz'):\n",
    "        import dipy.core.gradients as dpg\n",
    "        gtab = dpg.gradient_table(hardi_fbval, hardi_fbvec)\n",
    "        warped_hardi, mapping = reg.syn_register_dwi(hardi_fdata, gtab)\n",
    "        reg.write_mapping(mapping, f'./{subject}/mapping.nii.gz')\n",
    "    else:\n",
    "        mapping = reg.read_mapping(f'./{subject}/mapping.nii.gz', img, MNI_T2_img)\n",
    "\n",
    "\n",
    "    print(\"Tracking...\")\n",
    "    if not op.exists(f'./{subject}/dti_streamlines.trk'):\n",
    "        seed_roi = np.zeros(img.shape[:-1])\n",
    "        for name in bundle_names_afq:\n",
    "            for hemi in ['_R', '_L']:\n",
    "                for roi in bundles_afq[name + hemi]['ROIs']:\n",
    "                    warped_roi = patch_up_roi(\n",
    "                        (mapping.transform_inverse(\n",
    "                            roi.get_data().astype(np.float32),\n",
    "                         interpolation='linear')) > 0)\n",
    "\n",
    "                    # Add voxels that aren't there yet:\n",
    "                    seed_roi = np.logical_or(seed_roi, warped_roi)\n",
    "                \n",
    "        streamlines = aft.track(dti_params['params'], seed_mask=seed_roi,\n",
    "                                stop_mask=FA_data, stop_threshold=0.1)\n",
    "\n",
    "        sft = StatefulTractogram(streamlines, img, Space.RASMM)\n",
    "        save_tractogram(sft, f'./{subject}/dti_streamlines.trk',\n",
    "                        bbox_valid_check=False)\n",
    "    else:\n",
    "        tg = load_tractogram(f'./{subject}/dti_streamlines.trk', img)\n",
    "        streamlines = tg.streamlines\n",
    "\n",
    "    streamlines = dts.Streamlines(\n",
    "        dtu.transform_tracking_output(streamlines,\n",
    "                                      np.linalg.inv(img.affine)))\n",
    "\n",
    "    print(\"Segmenting...\")\n",
    "    segmentation_reco = seg.Segmentation(algo='reco',\n",
    "                                         model_clust_thr=20,\n",
    "                                         reduction_thr=20,\n",
    "                                         b0_threshold=50,\n",
    "                                         return_idx=True)\n",
    "    bundle_names_reco = ['CST',\n",
    "                         'UF',\n",
    "                         'AF',\n",
    "                         'IFOF']\n",
    "    bundles_reco = make_bundle_dict(bundle_names=bundle_names_reco, seg_algo='reco')\n",
    "    fiber_groups_reco = segmentation_reco.segment(bundles_reco, streamlines, hardi_fdata, hardi_fbval, hardi_fbvec,\n",
    "                                                  mapping=mapping, reg_template=MNI_T2_img)\n",
    "    print(len(fiber_groups_reco))\n",
    "\n",
    "    segmentation_afq = seg.Segmentation(algo='afq', return_idx=True)\n",
    "    fiber_groups_afq = segmentation_afq.segment(bundles_afq, streamlines, hardi_fdata, hardi_fbval, hardi_fbvec,\n",
    "                                                mapping=mapping, reg_template=MNI_T2_img)\n",
    "    print(len(fiber_groups_afq))\n",
    "\n",
    "    print(\"Cleaning...\")\n",
    "    for kk in fiber_groups_reco:\n",
    "        fiber_groups_reco[kk] = clean_fiber_group(fiber_groups_reco[kk])\n",
    "    for kk in fiber_groups_afq:\n",
    "        fiber_groups_afq[kk] = clean_fiber_group(fiber_groups_afq[kk])\n",
    "\n",
    "    print(\"Extracting tract profiles...\")\n",
    "    profiles = []\n",
    "    for kk in fiber_groups_reco:\n",
    "        weights = gaussian_weights(fiber_groups_reco[kk]['sl'])\n",
    "        profile = afq_profile(FA_data, fiber_groups_reco[kk]['sl'],\n",
    "                              np.eye(4), weights=weights)\n",
    "        for ii in range(len(profile)):\n",
    "            # Subject, Bundle, node, method, metric (FA, MD), value\n",
    "            profiles.append([subject, kk, ii, 'reco', 'FA', profile[ii]])\n",
    "\n",
    "    for kk in fiber_groups_afq:\n",
    "        weights = gaussian_weights(fiber_groups_afq[kk]['sl'])\n",
    "        profile = afq_profile(FA_data, fiber_groups_afq[kk]['sl'],\n",
    "                              np.eye(4), weights=weights)\n",
    "        for ii in range(len(profile)):\n",
    "            # Subject, Bundle, node, method, metric (FA, MD), value\n",
    "            profiles.append([subject, kk, ii, 'afq', 'FA', profile[ii]])\n",
    "\n",
    "    profiles = pd.DataFrame(data=profiles, columns=[\"Subject\", \"Bundle\", \"Node\", \"Method\", \"Metric\", \"Value\"])\n",
    "    profiles.to_csv(f\"./{subject}/profiles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is already in place. If you want to fetch it again please first remove the folder /Users/john/AFQ_data/templates \n",
      "Dataset is already in place. If you want to fetch it again please first remove the folder /Users/john/AFQ_data/callosum_templates \n",
      "Fetching HCP subject 100408\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c58a96cc84d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m             101006]\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msubject\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubjects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-dfa730cb5121>\u001b[0m in \u001b[0;36mprofile\u001b[0;34m(subject)\u001b[0m\n\u001b[1;32m     42\u001b[0m                   \u001b[0mprofile_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                   \u001b[0maws_access_key_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mak\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                   aws_secret_access_key=sk)  \n\u001b[0m\u001b[1;32m     45\u001b[0m     dwi_dir = op.join(afd.afq_home, 'HCP', 'derivatives',\n\u001b[1;32m     46\u001b[0m                       'dmriprep', f'sub-{subject}', 'sess-01/dwi')\n",
      "\u001b[0;32m~/pyAFQ/AFQ/data.py\u001b[0m in \u001b[0;36mfetch_hcp\u001b[0;34m(subjects, hcp_bucket, profile_name, path, aws_access_key_id, aws_secret_access_key)\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m             \u001b[0mbucket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m     \u001b[0;31m# Create the BIDS dataset description file text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     dataset_description = {\n",
      "\u001b[0;32m~/miniconda3/envs/afq/lib/python3.7/site-packages/boto3/s3/inject.py\u001b[0m in \u001b[0;36mbucket_download_file\u001b[0;34m(self, Key, Filename, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[1;32m    244\u001b[0m     return self.meta.client.download_file(\n\u001b[1;32m    245\u001b[0m         \u001b[0mBucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mKey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         ExtraArgs=ExtraArgs, Callback=Callback, Config=Config)\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/afq/lib/python3.7/site-packages/boto3/s3/inject.py\u001b[0m in \u001b[0;36mdownload_file\u001b[0;34m(self, Bucket, Key, Filename, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[1;32m    170\u001b[0m         return transfer.download_file(\n\u001b[1;32m    171\u001b[0m             \u001b[0mbucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mKey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             extra_args=ExtraArgs, callback=Callback)\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/afq/lib/python3.7/site-packages/boto3/s3/transfer.py\u001b[0m in \u001b[0;36mdownload_file\u001b[0;34m(self, bucket, key, filename, extra_args, callback)\u001b[0m\n\u001b[1;32m    305\u001b[0m             bucket, key, filename, extra_args, subscribers)\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0;31m# This is for backwards compatibility where when retries are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;31m# exceeded we need to throw the same error from boto3 instead of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/afq/lib/python3.7/site-packages/s3transfer/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/afq/lib/python3.7/site-packages/s3transfer/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;31m# however if a KeyboardInterrupt is raised we want want to exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;31m# out of this and propogate the exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/afq/lib/python3.7/site-packages/s3transfer/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# possible value integer value, which is on the scale of billions of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;31m# years...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_done_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAXINT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;31m# Once done waiting, raise an exception if present or return the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/afq/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/afq/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "subjects = [100408,\n",
    "            100610,\n",
    "            101006]\n",
    "for subject in subjects:\n",
    "    profile(subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles = pd.DataFrame(columns=[\"Subject\", \"Bundle\", \"Node\", \"Method\", \"Metric\", \"Value\"])\n",
    "for subject in subjects:\n",
    "    profiles.append(pd.read_csv(f\"./{subject}/profiles.csv\"))\n",
    "\n",
    "profiles_reco = profiles.loc[profiles[\"Method\"] == 'reco', 'Value']\n",
    "profiles_afq = profiles.loc[profiles[\"Method\"] == 'afq', 'Value']\n",
    "\n",
    "for i in range(len(profiles_reco)):\n",
    "    print(numpy.linalg.norm(profiles_reco[i]-profiles_afq[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
