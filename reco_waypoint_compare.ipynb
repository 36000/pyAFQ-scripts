{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hcp.recobundles/sub-2/sub-3_csd.nii.gz'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'hcp.recobundles' + f'/sub-{2}/sub-{3}_csd.nii.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is already in place. If you want to fetch it again please first remove the folder /Users/john/AFQ_data/templates \n",
      "Dataset is already in place. If you want to fetch it again please first remove the folder /Users/john/AFQ_data/callosum_templates \n",
      "{'IFO_R', 'CGC_L', 'FP', 'ATR_L', 'UNC_L', 'FA', 'CST_R', 'HCC_R', 'SLF_L', 'CST_L', 'ILF_R', 'ILF_L', 'IFO_L', 'ATR_R', 'SLF_R', 'HCC_L', 'ARC_R', 'UNC_R', 'CGC_R', 'ARC_L'}\n"
     ]
    }
   ],
   "source": [
    "from AFQ.api import make_bundle_dict\n",
    "bundles_waypoint_roi = set(make_bundle_dict().keys())\n",
    "print(bundles_waypoint_roi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is already in place. If you want to fetch it again please first remove the folder /Users/john/AFQ_data/hcp_atlas_16_bundles \n",
      "{'AF_L', 'F_R', 'CST_L', 'IFOF_R', 'F_L', 'UF_R', 'C_L', 'CST_R', 'UF_L', 'CCMid', 'MCP', 'whole_brain', 'CC_ForcepsMajor', 'CC_ForcepsMinor', 'AF_R', 'C_R', 'IFOF_L'}\n"
     ]
    }
   ],
   "source": [
    "from AFQ.data import read_hcp_atlas_16_bundles\n",
    "bundles_reco = set(read_hcp_atlas_16_bundles().keys())\n",
    "print(bundles_reco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CST_L', 'CST_R'}\n"
     ]
    }
   ],
   "source": [
    "print(bundles_waypoint_roi & bundles_reco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IFOF is IFO\n",
    "# UNC is UF\n",
    "# AF is ARC\n",
    "# CST is CST\n",
    "# FA and FP maybe related to F_L and F_R\n",
    "\n",
    "# F-1 score comparing set membership\n",
    "# dice coefficient between covered voxels (1 or 0 map)\n",
    "# compare bundle profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os.path as op\n",
    "\n",
    "CP = configparser.ConfigParser()\n",
    "CP.read_file(open(op.join(op.expanduser('~'), 'aws', 'credentials')))\n",
    "CP.sections()\n",
    "ak = CP.get('hcp', 'AWS_ACCESS_KEY_ID')\n",
    "sk = CP.get('hcp', 'AWS_SECRET_ACCESS_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def clean_fiber_group(streamlines, n_points=100, clean_rounds=5,\n",
    "                      clean_threshold=3, min_sl=20, stat=np.mean):\n",
    "    \"\"\"\n",
    "    Clean a segmented fiber group based on the Mahalnobis distance of\n",
    "    each streamline\n",
    "    Parameters\n",
    "    ----------\n",
    "    streamlines : nibabel.Streamlines class instance.\n",
    "        The streamlines constituting a fiber group.\n",
    "        If streamlines is None, will use previously given streamlines.\n",
    "        Default: None.\n",
    "    clean_rounds : int, optional.\n",
    "        Number of rounds of cleaning based on the Mahalanobis distance from\n",
    "        the mean of extracted bundles. Default: 5\n",
    "    clean_threshold : float, optional.\n",
    "        Threshold of cleaning based on the Mahalanobis distance (the units are\n",
    "        standard deviations). Default: 3.\n",
    "    min_sl : int, optional.\n",
    "        Number of streamlines in a bundle under which we will\n",
    "        not bother with cleaning outliers. Default: 20.\n",
    "    stat : callable, optional.\n",
    "        The statistic of each node relative to which the Mahalanobis is\n",
    "        calculated. Default: `np.mean` (but can also use median, etc.)\n",
    "    Returns\n",
    "    -------\n",
    "    A nibabel.Streamlines class instance containing only the streamlines\n",
    "    that have a Mahalanobis distance smaller than `clean_threshold` from\n",
    "    the mean of each one of the nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    # We don't even bother if there aren't enough streamlines:\n",
    "    if len(streamlines) < min_sl:\n",
    "        return streamlines\n",
    "\n",
    "    # Resample once up-front:\n",
    "    fgarray = _resample_bundle(streamlines[:]['sl'], n_points)\n",
    "    # Keep this around, so you can use it for indexing at the very end:\n",
    "    idx = np.arange(len(fgarray))\n",
    "    # This calculates the Mahalanobis for each streamline/node:\n",
    "    w = gaussian_weights(fgarray, return_mahalnobis=True, stat=stat)\n",
    "    # We'll only do this for clean_rounds\n",
    "    rounds_elapsed = 0\n",
    "    while (np.any(w > clean_threshold)\n",
    "           and rounds_elapsed < clean_rounds\n",
    "           and len(streamlines) > min_sl):\n",
    "        # Select the fibers that have Mahalanobis smaller than the\n",
    "        # threshold for all their nodes:\n",
    "        idx_belong = np.where(\n",
    "            np.all(w < clean_threshold, axis=-1))[0]\n",
    "        idx = idx[idx_belong.astype(int)]\n",
    "        # Update by selection:\n",
    "        fgarray = fgarray[idx_belong.astype(int)]\n",
    "        # Repeat:\n",
    "        w = gaussian_weights(fgarray, return_mahalnobis=True)\n",
    "        rounds_elapsed += 1\n",
    "    # Select based on the variable that was keeping track of things for us:\n",
    "    return streamlines[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyAFQ examples. Performs tractography on HARDI dataset.\n",
    "\"\"\"\n",
    "==========================\n",
    "Plotting tract profiles\n",
    "==========================\n",
    "\n",
    "An example of tracking and segmenting two tracts, and plotting their tract\n",
    "profiles for FA (calculated with DTI).\n",
    "\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import os.path as op\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import dipy.data as dpd\n",
    "from dipy.data import fetcher\n",
    "import dipy.tracking.utils as dtu\n",
    "import dipy.tracking.streamline as dts\n",
    "from dipy.io.streamline import save_tractogram, load_tractogram\n",
    "from dipy.stats.analysis import afq_profile, gaussian_weights\n",
    "from dipy.io.stateful_tractogram import StatefulTractogram\n",
    "from dipy.io.stateful_tractogram import Space\n",
    "\n",
    "import AFQ.utils.streamlines as aus\n",
    "import AFQ.data as afd\n",
    "import AFQ.tractography as aft\n",
    "import AFQ.registration as reg\n",
    "import AFQ.dti as dti\n",
    "import AFQ.segmentation as seg\n",
    "from AFQ.utils.volume import patch_up_roi\n",
    "from AFQ.api import make_bundle_dict\n",
    "\n",
    "def profile(subject):\n",
    "    bundle_names_afq = ['CST',\n",
    "                        'UNC',\n",
    "                        'ARC',\n",
    "                        'IFO']\n",
    "    bundles_afq = make_bundle_dict(bundle_names=bundle_names_afq, seg_algo='afq')\n",
    "    print(f\"Fetching HCP subject {subject}\")\n",
    "    afd.fetch_hcp([subject], \n",
    "                  profile_name=False,\n",
    "                  aws_access_key_id=ak,\n",
    "                  aws_secret_access_key=sk)  \n",
    "    dwi_dir = op.join(afd.afq_home, 'HCP', 'derivatives',\n",
    "                      'dmriprep', f'sub-{subject}', 'sess-01/dwi')\n",
    "\n",
    "    hardi_fdata = op.join(dwi_dir, f\"sub-{subject}_dwi.nii.gz\")\n",
    "    hardi_fbval = op.join(dwi_dir, f\"sub-{subject}_dwi.bval\")\n",
    "    hardi_fbvec = op.join(dwi_dir, f\"sub-{subject}_dwi.bvec\")\n",
    "\n",
    "    img = nib.load(hardi_fdata)\n",
    "\n",
    "    print(\"Calculating DTI...\")\n",
    "    if not op.exists(f'./{subject}/dti_FA.nii.gz'):\n",
    "        dti_params = dti.fit_dti(hardi_fdata, hardi_fbval, hardi_fbvec,\n",
    "                                 out_dir=f'./{subject}')\n",
    "    else:\n",
    "        dti_params = {'FA': f'./{subject}/dti_FA.nii.gz',\n",
    "                      'params': f'./{subject}/dti_params.nii.gz'}\n",
    "\n",
    "    FA_img = nib.load(dti_params['FA'])\n",
    "    FA_data = FA_img.get_fdata()\n",
    "\n",
    "    templates = afd.read_templates()\n",
    "    bundles = make_bundle_dict()\n",
    "\n",
    "    print(\"Registering to template...\")\n",
    "    MNI_T2_img = dpd.read_mni_template()\n",
    "    if not op.exists(f'./{subject}/mapping.nii.gz'):\n",
    "        import dipy.core.gradients as dpg\n",
    "        gtab = dpg.gradient_table(hardi_fbval, hardi_fbvec)\n",
    "        warped_hardi, mapping = reg.syn_register_dwi(hardi_fdata, gtab)\n",
    "        reg.write_mapping(mapping, f'./{subject}/mapping.nii.gz')\n",
    "    else:\n",
    "        mapping = reg.read_mapping(f'./{subject}/mapping.nii.gz', img, MNI_T2_img)\n",
    "\n",
    "\n",
    "    print(\"Tracking...\")\n",
    "    if not op.exists(f'./{subject}/dti_streamlines.trk'):\n",
    "        seed_roi = np.zeros(img.shape[:-1])\n",
    "        for name in bundle_names_afq:\n",
    "            for hemi in ['_R', '_L']:\n",
    "                for roi in bundles_afq[name + hemi]['ROIs']:\n",
    "                    warped_roi = patch_up_roi(\n",
    "                        (mapping.transform_inverse(\n",
    "                            roi.get_data().astype(np.float32),\n",
    "                         interpolation='linear')) > 0)\n",
    "\n",
    "                    # Add voxels that aren't there yet:\n",
    "                    seed_roi = np.logical_or(seed_roi, warped_roi)\n",
    "                \n",
    "        streamlines = aft.track(dti_params['params'], seed_mask=seed_roi,\n",
    "                                stop_mask=FA_data, stop_threshold=0.1)\n",
    "\n",
    "        sft = StatefulTractogram(streamlines, img, Space.RASMM)\n",
    "        save_tractogram(sft, f'./{subject}/dti_streamlines.trk',\n",
    "                        bbox_valid_check=False)\n",
    "    else:\n",
    "        tg = load_tractogram(f'./{subject}/dti_streamlines.trk', img)\n",
    "        streamlines = tg.streamlines\n",
    "\n",
    "    streamlines = dts.Streamlines(\n",
    "        dtu.transform_tracking_output(streamlines,\n",
    "                                      np.linalg.inv(img.affine)))\n",
    "\n",
    "    print(\"Segmenting...\")\n",
    "    segmentation_reco = seg.Segmentation(algo='reco',\n",
    "                                         model_clust_thr=20,\n",
    "                                         reduction_thr=20,\n",
    "                                         b0_threshold=50,\n",
    "                                         return_idx=True)\n",
    "    bundle_names_reco = ['CST',\n",
    "                         'UF',\n",
    "                         'AF',\n",
    "                         'IFOF']\n",
    "    bundles_reco = make_bundle_dict(bundle_names=bundle_names_reco, seg_algo='reco')\n",
    "    fiber_groups_reco = segmentation_reco.segment(bundles_reco, streamlines, hardi_fdata, hardi_fbval, hardi_fbvec,\n",
    "                                                  mapping=mapping, reg_template=MNI_T2_img)\n",
    "    print(len(fiber_groups_reco))\n",
    "\n",
    "    segmentation_afq = seg.Segmentation(algo='afq', return_idx=True)\n",
    "    fiber_groups_afq = segmentation_afq.segment(bundles_afq, streamlines, hardi_fdata, hardi_fbval, hardi_fbvec,\n",
    "                                                mapping=mapping, reg_template=MNI_T2_img)\n",
    "    print(len(fiber_groups_afq))\n",
    "\n",
    "    print(\"Cleaning...\")\n",
    "    for kk in fiber_groups_reco:\n",
    "        fiber_groups_reco[kk] = clean_fiber_group(fiber_groups_reco[kk])\n",
    "    for kk in fiber_groups_afq:\n",
    "        fiber_groups_afq[kk] = clean_fiber_group(fiber_groups_afq[kk])\n",
    "\n",
    "    print(\"Extracting tract profiles...\")\n",
    "    profiles = []\n",
    "    for kk in fiber_groups_reco:\n",
    "        weights = gaussian_weights(fiber_groups_reco[kk]['sl'])\n",
    "        profile = afq_profile(FA_data, fiber_groups_reco[kk]['sl'],\n",
    "                              np.eye(4), weights=weights)\n",
    "        for ii in range(len(profile)):\n",
    "            # Subject, Bundle, node, method, metric (FA, MD), value\n",
    "            profiles.append([subject, kk, ii, 'reco', 'FA', profile[ii]])\n",
    "\n",
    "    for kk in fiber_groups_afq:\n",
    "        weights = gaussian_weights(fiber_groups_afq[kk]['sl'])\n",
    "        profile = afq_profile(FA_data, fiber_groups_afq[kk]['sl'],\n",
    "                              np.eye(4), weights=weights)\n",
    "        for ii in range(len(profile)):\n",
    "            # Subject, Bundle, node, method, metric (FA, MD), value\n",
    "            profiles.append([subject, kk, ii, 'afq', 'FA', profile[ii]])\n",
    "\n",
    "    profiles = pd.DataFrame(data=profiles, columns=[\"Subject\", \"Bundle\", \"Node\", \"Method\", \"Metric\", \"Value\"])\n",
    "    profiles.to_csv(f\"./{subject}/profiles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is already in place. If you want to fetch it again please first remove the folder C:\\Users\\john\\AFQ_data\\templates \n",
      "Dataset is already in place. If you want to fetch it again please first remove the folder C:\\Users\\john\\AFQ_data\\callosum_templates \n",
      "Fetching HCP subject 100408\n",
      "Calculating DTI...\n",
      "Dataset is already in place. If you want to fetch it again please first remove the folder C:\\Users\\john\\AFQ_data\\templates \n",
      "Dataset is already in place. If you want to fetch it again please first remove the folder C:\\Users\\john\\AFQ_data\\templates \n",
      "Dataset is already in place. If you want to fetch it again please first remove the folder C:\\Users\\john\\AFQ_data\\callosum_templates \n",
      "Registering to template...\n",
      "Data size is approximately 70MB\n",
      "Dataset is already in place. If you want to fetch it again please first remove the folder C:\\Users\\john\\.dipy\\mni_template \n",
      "Tracking...\n",
      "Segmenting...\n",
      "Dataset is already in place. If you want to fetch it again please first remove the folder C:\\Users\\john\\AFQ_data\\hcp_atlas_16_bundles \n",
      " Resampled to 20 points\n",
      " Size is 28.353 MB\n",
      " Duration of resampling is 0.076 sec.\n",
      " QBX phase starting...\n",
      " Merging phase starting ...\n",
      " QuickBundlesX time for 123874 random streamlines\n",
      " Duration 2.432 sec. \n",
      "\n",
      " Resampled to 20 points\n",
      " Size is 17.750 MB\n",
      " Duration of resampling is 0.025 sec.\n",
      " QBX phase starting...\n",
      " Merging phase starting ...\n",
      " QuickBundlesX time for 38776 random streamlines\n",
      " Duration 1.057 sec. \n",
      "\n",
      "Progressive Registration is Enabled\n",
      " Translation  (3 parameters)...\n",
      " Rigid  (6 parameters) ...\n",
      " Similarity (7 parameters) ...\n",
      " Scaling (9 parameters) ...\n",
      " Affine (12 parameters) ...\n",
      "8\n",
      "Cleaning...\n",
      "Extracting tract profiles...\n",
      "188\n",
      "36\n",
      "78\n",
      "442\n",
      "203\n",
      "30\n",
      "16\n",
      "86\n",
      "Dataset is already in place. If you want to fetch it again please first remove the folder C:\\Users\\john\\AFQ_data\\templates \n",
      "Dataset is already in place. If you want to fetch it again please first remove the folder C:\\Users\\john\\AFQ_data\\callosum_templates \n",
      "Fetching HCP subject 100610\n",
      "Calculating DTI...\n",
      "Dataset is already in place. If you want to fetch it again please first remove the folder C:\\Users\\john\\AFQ_data\\templates \n",
      "Dataset is already in place. If you want to fetch it again please first remove the folder C:\\Users\\john\\AFQ_data\\templates \n",
      "Dataset is already in place. If you want to fetch it again please first remove the folder C:\\Users\\john\\AFQ_data\\callosum_templates \n",
      "Registering to template...\n",
      "Data size is approximately 70MB\n",
      "Dataset is already in place. If you want to fetch it again please first remove the folder C:\\Users\\john\\.dipy\\mni_template \n",
      "Data size is approximately 70MB\n",
      "Dataset is already in place. If you want to fetch it again please first remove the folder C:\\Users\\john\\.dipy\\mni_template \n",
      "Creating scale space from the moving image. Levels: 3. Sigma factor: 0.200000.\n",
      "Creating scale space from the static image. Levels: 3. Sigma factor: 0.200000.\n",
      "Optimizing level 2\n",
      "Optimizing level 1\n",
      "Optimizing level 0\n",
      "Tracking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [02:46<00:00,  3.21it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 106326/106326 [04:50<00:00, 365.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting...\n",
      "Dataset is already in place. If you want to fetch it again please first remove the folder C:\\Users\\john\\AFQ_data\\hcp_atlas_16_bundles \n",
      " Resampled to 20 points\n",
      " Size is 28.353 MB\n",
      " Duration of resampling is 0.083 sec.\n",
      " QBX phase starting...\n",
      " Merging phase starting ...\n",
      " QuickBundlesX time for 123874 random streamlines\n",
      " Duration 2.515 sec. \n",
      "\n",
      " Resampled to 20 points\n",
      " Size is 19.869 MB\n",
      " Duration of resampling is 0.028 sec.\n",
      " QBX phase starting...\n",
      " Merging phase starting ...\n",
      " QuickBundlesX time for 43405 random streamlines\n",
      " Duration 1.594 sec. \n",
      "\n",
      "Progressive Registration is Enabled\n",
      " Translation  (3 parameters)...\n",
      " Rigid  (6 parameters) ...\n",
      " Similarity (7 parameters) ...\n",
      " Scaling (9 parameters) ...\n",
      " Affine (12 parameters) ...\n",
      " You have removed all streamlines\n",
      " You have removed all streamlines\n"
     ]
    }
   ],
   "source": [
    "subjects = [100408,\n",
    "            100610,\n",
    "            101006]\n",
    "for subject in subjects:\n",
    "    profile(subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles = pd.DataFrame(columns=[\"Subject\", \"Bundle\", \"Node\", \"Method\", \"Metric\", \"Value\"])\n",
    "for subject in subjects:\n",
    "    profiles.append(pd.read_csv(f\"./{subject}/profiles.csv\"))\n",
    "\n",
    "profiles_reco = profiles.loc[profiles[\"Method\"] == 'reco', 'Value']\n",
    "profiles_afq = profiles.loc[profiles[\"Method\"] == 'afq', 'Value']\n",
    "\n",
    "for i in range(len(profiles_reco)):\n",
    "    print(numpy.linalg.norm(profiles_reco[i]-profiles_afq[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
